{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/suyash/Desktop/projects/Intent-classification-'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/suyash/Desktop/projects/Intent-classification-'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ic.constants import *\n",
    "from src.ic.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_path=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_path)\n",
    "        self.paramss=read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self)-> ModelTrainerConfig:\n",
    "        config=self.config.model_trainer\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config=ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.train_path,\n",
    "            model_ckpt=config.model_ckpt\n",
    "            \n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig, dataset):\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def train(self):\n",
    "        # Load model\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_ckpt, num_labels=self.config.num_labels\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.config.learning_rate)\n",
    "        loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metrics = [\"accuracy\"]\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # Prepare dataset\n",
    "        train_size = int(0.8 * len(list(self.dataset)))  # 80% for training\n",
    "        train_dataset = self.dataset.take(train_size).batch(self.config.batch_size)\n",
    "        val_dataset = self.dataset.skip(train_size).batch(self.config.batch_size)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(train_dataset, validation_data=val_dataset, epochs=self.config.epochs)\n",
    "\n",
    "        # Save model\n",
    "        model.save_pretrained(self.config.model_save_path)\n",
    "        print(f\"Model saved at {self.config.model_save_path}\")\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyash/Desktop/projects/Intent-classification-/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config_path, train_dataset, val_dataset, num_labels):\n",
    "        # Load configuration\n",
    "        with open(config_path, \"r\") as file:\n",
    "            self.config = yaml.safe_load(file)[\"model_trainer\"]\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.num_labels = num_labels  # Set dynamically\n",
    "        self.model_ckpt = self.config[\"model_ckpt\"]\n",
    "        self.epochs = self.config[\"epochs\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.model_save_path = self.config[\"model_save_path\"]\n",
    "\n",
    "        # Ensure model save directory exists\n",
    "        os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Initialize, compile, train, and save the model.\"\"\"\n",
    "        # Load DeBERTa model\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_ckpt, num_labels=self.num_labels\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metrics = [\"accuracy\"]\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(self.train_dataset.batch(self.batch_size),\n",
    "                  validation_data=self.val_dataset.batch(self.batch_size),\n",
    "                  epochs=self.epochs)\n",
    "\n",
    "        # Save trained model\n",
    "        model.save_pretrained(self.model_save_path)\n",
    "        print(f\"✅ Model saved at {self.model_save_path}\")\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset='artifacts/data_transformation/transformed_data.csv'\n",
    "val_dataset='artifacts/data_ingestions/transformed_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data split complete: Train size = 5232, Validation size = 1308\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load transformed dataset\n",
    "df = pd.read_csv(\"artifacts/data_transformation/transformed_data.csv\")\n",
    "\n",
    "# Split into train (80%) and validation (20%)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the split datasets (optional)\n",
    "train_df.to_csv(\"artifacts/data_transformation/train.csv\", index=False)\n",
    "val_df.to_csv(\"artifacts/data_transformation/val.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Data split complete: Train size = {len(train_df)}, Validation size = {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_labels = 27 \n",
    "trainer = ModelTrainer(\"config/config.yaml\", train_dataset, val_dataset, num_labels)\n",
    "trained_model = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset='artifacts/data_transformation/train.csv'\n",
    "val_dataset='artifacts/data_transformation/val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/suyash/Desktop/projects/Intent-classification-'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
